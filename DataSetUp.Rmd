---
title: "DataSetUp"
author: "Zoe Searcy (Schroder)"
date: '2023-06-02'
output: html_document
---


Projections you may need: 
```{r}
merc <- "+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs"
US_LCC <- "+proj=lcc +lat_1=33 +lat_2=45 +lat_0=39 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m no_defs"
```

Coordinate Reference Systems you may need:

https://spatialreference.org/ref/esri/usa-contiguous-lambert-conformal-conic/
```{r}
WGS84 <- 4326
```

Install the packages for this project: 

```{r}
suppressMessages(library(sf))
suppressMessages(library(dplyr))
suppressMessages(library(lubridate))
suppressMessages(library(lutz))
suppressMessages(library(xts))
suppressMessages(library(chron))
suppressMessages(library(sp))
```

##########################
## Part 1: Tornado Data ##
##########################

The newest GIS shapefile contains missing geometries for more than 30% of the tornadoes. The number of missing geometries is highest after 1995. Instead here we use the csv file from https://www.spc.noaa.gov/wcm/#data  Use the start lon/lat and create a `sp` object then convert to `sf`. Set the coordinate reference system (crs) to ESPG 4326.
```{r, eval = FALSE}
Tor.spdf <- read.csv(file = "1950-2022_actual_tornadoes.csv")
sp::coordinates(Tor.spdf) <- ~ slon + slat
Tor.sfdf <- st_as_sf(Tor.spdf)
st_crs(Tor.sfdf) <- 4326
```

Remove tornadoes in Hawaii, Alaska, and Puerto Rico and those occurring before 1994. That year marks the beginning of comprehensive WSR-88D radar. For missing EF ratings use the modification rules (if/else) defined here: https://www.spc.noaa.gov/wcm/OneTor_F-scale-modifications.pdf 
```{r, eval = FALSE}
All_Tornadoes <- Tor.sfdf %>%
  filter(yr >= 1994,
         !st %in% c("AK", "PR", "HI")) %>%
  mutate(mag = ifelse(mag == -9 & len <= 5, 0, mag),
         mag = ifelse(mag == -9 & len > 5, 1, mag))
```

Add a data/time column also add columns for path length, width, and area in metric units. Leave the time zone as native CDT. Create a convective day (6AM to 6AM) column taking hours 00:00:00 -> 05:59:59 and assigning it to the previous date (this associates the previous day's date to tornadoes occurring up to 6 hours after local midnight).
```{r, eval = FALSE}
All_Tornadoes <- All_Tornadoes %>%
  mutate(#dy = format(as.Date(date,format="%y/%m/%d"), "%d"), #This is now an included column in data
         DateTime = as.POSIXct(paste(yr, mo, dy, time), format = "%Y%m%d%H:%M:%S"),
         Hour = hour(DateTime),
         Year = year(DateTime),
         cDateTime = DateTime - as.difftime(6, unit = "hours"),
         cDate = as.Date(as_datetime(ifelse(Hour < 6, (DateTime - 86400), cDateTime), tz = Sys.timezone())),
         Length = len * 1609.34,
         Length = ifelse(Length == 0, min(Length[Length > 0]), Length), #takes care of zero length
         Width = wid * .9144,
         Width = ifelse(Width == 0, min(Width[Width > 0]), Width), #takes care of zero width
         Width = ifelse(Year >= 1995, Width * pi/4, Width), #takes care of change: avg to max
         cas = inj + fat,
         AreaPath = Length * Width,
         Ma = factor(month.abb[mo], levels = month.abb[1:12])) %>%
  sf::st_sf()
dim(All_Tornadoes)[1]
```

The geometry type is `POINT`. Each tornado is represented as a single point location geometry (start location). Add power dissipation per tornado.

Add power dissipation per tornado. Use the empirical model for tornado winds by EF rating taken from Table 3-1 of NRC 2007. Percent area by EF rating for each EF category. Threshold wind speeds (m/s) are a lower bound 3-sec gusts on the operational EF Scale (Table 2-1 of NRC2007). This is based on work by Fricker et al. (2017). The model is
$$
E = A_p \rho \sum_{j=0}^{J} w_j v_j^{3},
$$
where $A_p$ is the area of the path, $\rho$ is area density [1 kg/m^3]  $v_j$ is the midpoint wind speed for each rating, and $w_j$ is the corresponding fraction of path area by EF rating. With no upper bound on the EF5 wind speeds, the midpoint wind speed is set at 97 m~s$^{-1}$ (7.5 m~s$^{-1}$ above the threshold wind speed consistent with the EF4 midpoint speed relative to its threshold)
```{r, eval = FALSE}
perc <- c(1, 0, 0, 0, 0, 0, 
          .772, .228, 0, 0, 0, 0,
          .616, .268, .115, 0, 0, 0,
          .529, .271, .133, .067, 0, 0,
          .543, .238, .131, .056, .032, 0,
          .538, .223, .119, .07, .033, .017)
percM <- matrix(perc, ncol = 6, byrow = TRUE)
threshW <- c(29.06, 38.45, 49.62, 60.8, 74.21, 89.41)
midptW <- c(diff(threshW)/2 + threshW[-length(threshW)], threshW[length(threshW)] + 7.5)
ef <- All_Tornadoes$mag + 1
EW3 <- numeric()
for(i in 1:length(ef)) EW3[i] = midptW^3 %*% percM[ef[i], ]
All_Tornadoes <- All_Tornadoes %>%
  mutate(ED = EW3 * AreaPath)
``` 


```{r, eval = FALSE}
TornHalf <- All_Tornadoes %>%
  filter(yr <= 2008)

TornHalf2 <- All_Tornadoes %>%
  filter(yr >= 2008)
```


Determine the distance between tornadoes in space and time. Use a projection, not lat/lon. See https://epsg.io/102004. Extract the coordinates of the start locations as a N by 2 matrix, where N is the number of tornadoes. Also extract the date-time as a vector of class `POSIXct`.
```{r, eval = FALSE}
TornHalf <- st_transform(TornHalf, crs = US_LCC)
space <- st_coordinates(TornHalf)
time <- TornHalf$DateTime
```

Next compute pairwise Euclidean distances in space and, separately, in time using the `dist()` function. Divide the spatial distance by 15 so that the values are commensurate with the time 'distance' based on the assumption of 15 meters per second (~34 mph) for an average speed of tornado-generating storms. Compare: Distance from New York to Denver is 2.622 x 10^6 meters. There are 3.154 x 10^7 seconds in a year. This will capture the historic multiday tornado outbreaks. For analysis we want to consider each day in the multiday group separately. As the value of the divisor increases cluster areas get larger. Remove `ds` and `dt` to free memory. Distances are saved as an object of class `dist` containing a vector of length N * (N-1)/2, which is the number of unique point pairs.
```{r, eval = FALSE}
ds <- dist(space) / 15
dt <- dist(time)
dst <- ds + dt
rm(ds, dt)
```

Distances are saved as an object of class `dist` containing a vector of length N * (N-1)/2, which is the number of unique point pairs.

Next group the tornadoes based on the space-time distances. This is done with the `hclust()` (hierarchical cluster) function. Initially, each tornado is assigned to its own group and then the algorithm joins the two closest tornadoes determined by values in `dst`. The algorithm continues by joining tornadoes (and tornado groups) until there is a single large group.

The single linkage method (`method = "single"`) is related to the minimal spanning tree (MST) and adopts a 'friends of friends' grouping strategy. An edge-weighted graph is a graph where each edge has a weight (or cost). Here weights are space-time distances between tornadoes. A MST of an edge-weighted graph is a spanning tree whose weight (the sum of the weights of its edges) is no larger than the weight of any other spanning tree. A spanning tree of a graph on N vertices (tornado centroids) is a subset of N-1 edges that form a tree (Skiena 1990, p. 227).
 
The `cutree()` function is used to extract a group number for each tornado. Tornadoes in each group are close in space & time. Here the tree is cut at a height of 50000 space-time units. Making `h` smaller results in smaller groups (fewer tornadoes per group).
```{r, eval = FALSE}
stime <- proc.time()
tree <- hclust(dst, method = "single")
groupNumber <- as.integer(cutree(tree, h = 50000))
proc.time() - stime
```

Add the group number to each tornado. 
```{r, eval = FALSE}
TornHalf$groupNumber <- groupNumber
```

```{r, eval = FALSE}
TornHalf2 <- st_transform(TornHalf2, crs = US_LCC)
space <- st_coordinates(TornHalf2)
time <- TornHalf2$DateTime

ds <- dist(space) / 15
dt <- dist(time)
dst <- ds + dt
rm(ds, dt, space, time)

stime <- proc.time()
tree <- hclust(dst, method = "single")
groupNumber <- as.integer(cutree(tree, h = 50000))
proc.time() - stime
rm(dst)
```

```{r, eval = FALSE}
dat <- TornHalf %>%
  filter(yr == 2007)
max(dat$groupNumber)

TornHalf2 <- TornHalf2 %>%
  mutate(groupNumber = groupNumber + 3868)
```

```{r, eval = FALSE}
TornHalf <- TornHalf %>%
  filter(yr <= 2007)

All_Tornadoes <- rbind(TornHalf, TornHalf2)
```

Compute group-level statistics. 
```{r, eval = FALSE}
Groups.sfdfT <- All_Tornadoes %>%
  group_by(groupNumber) %>%
  summarize(Year = first(Year),
            Month = first(mo),
            FirstDate = first(date),
            LastDate = last(date),
            Name = paste(FirstDate, "to", LastDate),
            FirstcDate = first(cDate),
            LastcDate = last(cDate),
            ncD = n_distinct(cDate),
            nT = n(),
            n0 = sum(mag == 0, na.rm = TRUE),
            n1 = sum(mag == 1, na.rm = TRUE),
            n2 = sum(mag == 2, na.rm = TRUE),
            n3 = sum(mag == 3, na.rm = TRUE),
            n4 = sum(mag == 4, na.rm = TRUE),
            n5 = sum(mag == 5, na.rm = TRUE),
            maxEF = max(mag),
            ATP = sum(ED, na.rm = TRUE),
            ATP_TW = ATP/10^12,
            maxEF = max(mag),
            nD = n_distinct(date),
            StartTime = first(DateTime),
            EndTime = last(DateTime),
            Duration = difftime(EndTime, StartTime, units = "secs"), 
            cas = sum(inj + fat)) 
```
 
```{r, eval = FALSE}
GroupTornadoes <- All_Tornadoes %>%
  filter(groupNumber %in% Groups.sfdfT$groupNumber)
```

##################################
## Extract Big Days from Groups ##
##################################

Filter individual tornadoes to remove those that are not part of a large group. Group by group number and convective dates. Remove days having fewer than 10 tornadoes.
```{r, eval = FALSE}
BigDays.sfdfT <- All_Tornadoes %>%
  filter(groupNumber %in% Groups.sfdfT$groupNumber) %>%
  group_by(groupNumber, cDate) %>%
  summarize(nT = n(),
            n0 = sum(mag == 0, na.rm = TRUE),
            n1 = sum(mag == 1, na.rm = TRUE),
            n2 = sum(mag == 2, na.rm = TRUE),
            n3 = sum(mag == 3, na.rm = TRUE),
            n4 = sum(mag == 4, na.rm = TRUE),
            n5 = sum(mag == 5, na.rm = TRUE),
            maxEF = max(mag),
            ATP = sum(ED, na.rm = TRUE),
            maxATP = max(ED),
            avgATP = mean(ED, na.rm = TRUE),
            GroupDayCas = sum(cas, na.rm = TRUE),
            GroupDayFat = sum(fat, na.rm = TRUE),
            StartTime_CST = first(DateTime),
            EndTime_CST= last(DateTime),
            StartTime_UTC = StartTime_CST + 21600,
            EndTime_UTC = EndTime_CST + 21600,
            Duration = difftime(EndTime_CST, StartTime_CST, units = "secs")) %>%
  filter(nT >= 10) %>%
  mutate(Year = year(cDate),
         Mo = month(cDate),
         Month = format(cDate, "%m"), # this is needed to preserve the leading zeros
         Day = format(cDate, "%d"), 
         ATP_TW = ATP/10^12)                                                                              
dim(BigDays.sfdfT)
```

Create a unique ID for each big day and each tornado. Extract the tornadoes associated with each big day using the unique ID.
```{r, eval = FALSE}
BigDayTornadoes <- All_Tornadoes %>%
   mutate(ID = paste0(gsub("-", "", cDate), groupNumber))
BigDays.sfdfT <- BigDays.sfdfT %>%
   mutate(ID = paste0(gsub("-", "", cDate), groupNumber))

BigDayTornadoes <- BigDayTornadoes %>%
  filter(ID %in% BigDays.sfdfT$ID)

sum(BigDays.sfdfT$nT)
```

Round the UTC time to nearest 6 hours. This is done with the `align.time()` function from the **xts** package. Adjust it by 3 hours to get the closest time. This falls within the outbreak so you need to subtract by 3 hours (10800 seconds). This will produce the closest 3 hour NARR time that occurs before and not within the big day. 
```{r, eval = FALSE}
BigDays.sfdfT$StartTime_UTC <- force_tz(BigDays.sfdfT$StartTime_UTC, tzone = "UTC")
BigDays.sfdfT$NARRtime <- (align.time(BigDays.sfdfT$StartTime_UTC, n = (60 * 60 * 3)) - 3600 * 3)
```


Split the NARR date and time into their individual variables. Then bind the columns for BigDays.sfdfT. NOTE: cannot do a mutate because 00Z produces NAs. DON'T USE!
```{r, eval = FALSE}
NARRday = format(as.POSIXct(strptime(BigDays.sfdfT$StartTime_UTC,"%Y-%m-%d %H:%M:%S",tz="")) ,format = "%Y/%m/%d")
NARRday[is.na(NARRday)] <- "2001/04/22"

NARRZtime = format(as.POSIXct(strptime(BigDays.sfdfT$NARRtime,"%Y-%m-%d %H:%M:%S",tz="")) ,format = "%H")
NARRZtime[is.na(NARRZtime)] <- "00"

BigDays.sfdfT <- cbind(BigDays.sfdfT, NARRday, NARRZtime)
```

Create a table to show how many big days fall in each start Z time. 
```{r, eval = FALSE}
BigDays.sfdfT %>%
  group_by(NARRZtime) %>%
  summarize(count = n())
```

Create a downloadable string of information for the varying NARR times. 
```{r, eval = FALSE}
BigDays.sfdfT <- BigDays.sfdfT %>%
  mutate(YrMoDa = gsub("/", "", NARRday),
         slug = paste0("merged_AWIP32.",YrMoDa, NARRZtime),
         slug2 = paste0("merged_AWIP32.",YrMoDa))
```

Extract a vector of the big days. Save as a .csv for NARR download. 
```{r, eval = FALSE}
bigdays <- BigDays.sfdfT$NARRday
bigdaytimes <- BigDays.sfdfT$NARRZtime
x <- cbind(as.character(bigdays), as.character(bigdaytimes))
write.csv(x, "BigDays.csv")
```

```{r, eval = FALSE}
BigDays.sfdfT <- st_transform(BigDays.sfdfT, 
  crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
```

Obtain the group day hulls. Transform the CRS to match that of the environmental data raster grids.
```{r, eval = FALSE}
BigDays.sfdfT <- st_convex_hull(BigDays.sfdfT)
BigDays.sfdfT$HullArea <- st_area(BigDays.sfdfT)
BigDays.sfdfT <- st_transform(BigDays.sfdfT, 
  crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
```

Get the centroid (central point of the tornado activity) for each big day. 
```{r, eval = FALSE}
BigDayCentroids.df <- st_centroid(BigDays.sfdfT)
BigDayCentroids.df$groupArea <- st_area(st_convex_hull(BigDays.sfdfT))
BigDayCentroids.df$groupDensity <- BigDayCentroids.df$nT/BigDayCentroids.df$groupArea
```

## Download NARR data: 

Data is downloaded from NCAR's North American Regional Reanalysis (https://rda.ucar.edu/datasets/ds608.0/#!access). It extends from 1-1-1979 to 11-1-2018. Use the NCAR NARR 3-hourly files.  

Spatial Extent: 
Longitude Range: Westernmost = 148.64E Easternmost = 2.568W
Latitude Range: Southernmost = 0.897N Northernmost = 85.333N

```{r, eval = FALSE}
BigDays.sfdfT <- st_transform(BigDays.sfdfT, 
  crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
```

```{r}
#save(BigDays.sfdfT, BigDayTornadoes, BigDayCentroids.df, Groups.sfdfT, GroupTornadoes, All_Tornadoes, file = "TornadoOutbreaks.RData")
```

```{r}
#load("TornadoOutbreaks.RData")
```

###############
## NARR Data ##
###############

The list of all variables can be found here: http://www.emc.ncep.noaa.gov/mmb/rreanl/merged_land_AWIP32.pdf 
```{r}
slug <- BigDays.sfdfT$slug
slug2 <- BigDays.sfdfT$slug2
```

```{r}
library(raster)
```

Check the raster layers for accuracy: 
```{r}
i = 832
rb <- brick(paste0("/Users/zoesearcy/Desktop/ResearchProjects/TornEnvironmentsandGHGs/NARR21_22/", BigDays.sfdfT$slug2[i], "/",BigDays.sfdfT$slug[i]))
  
  #"/Volumes/Work/NCARNARR/All/", BigDays.sfdfT$slug2[i], "/",BigDays.sfdfT$slug[i]))
x <- as.list(rb)
x[[375]]
```

Read the grib files as raster bricks and assign the CAPE and helicity variables to separate raster layers. Extract the average (and extreme) environmental values within each of the big days in large groups hulls.
```{r, eval = FALSE}
avgCAPE <- numeric()
avgsbCAPE <- numeric()
avgDEW <- numeric()
avgMR <- numeric()
avgHLCY <- numeric()
avgCIN <- numeric()
avgsbCIN <- numeric()
avgUSTM <- numeric()
avgVSTM <- numeric()
avgBS_deep <- numeric()
avgBS_shallow <- numeric()
avgSM <- numeric()
avgRATIO <- numeric()
avgLCL <- numeric()
maxCAPE <- numeric()
maxsbCAPE <- numeric()
maxDEW <- numeric()
maxMR <- numeric()
maxHLCY <- numeric()
minCIN <- numeric()
minsbCIN <- numeric()
maxUSTM <- numeric()
maxVSTM <- numeric()
maxBS_deep <- numeric()
maxBS_shallow <- numeric()
maxSM <- numeric()
maxLCL <- numeric()
minLCL <- numeric()
 
for(i in 1:length(slug)){
  print(i)
  #On Zoe's Mac: 
  rb <- brick(paste0("/Volumes/Work/NCARNARR/All/", BigDays.sfdfT$slug2[i], "/",BigDays.sfdfT$slug[i])) #<-- this is for varying NARR times
  CAPE <- raster(rb, layer = 375) 
  sbCAPE <- raster(rb, layer = 315) 
  DEW <- raster(rb, layer = 290)
  MR <- raster(rb, layer = 289)
  HLCY <- raster(rb, layer = 323) 
  CIN <- raster(rb, layer = 376) 
  sbCIN <- raster(rb, layer = 316) 
  USTM <- raster(rb, layer = 324) 
  VSTM <- raster(rb, layer = 325) 
  UGRD500 <- raster(rb, layer = 117) 
  VGRD500 <- raster(rb, layer = 118) 
  UGRD850 <- raster(rb, layer = 206) 
  VGRD850 <- raster(rb, layer = 207)
  UGRDsfc <- raster(rb, layer = 293) 
  VGRDsfc <- raster(rb, layer = 294)  
  LCL <- raster(rb, layer = 318) 
  SM <- sqrt(USTM^2 + VSTM^2)
  RATIO <- CAPE/abs(CIN)
  BS_deep <- sqrt(((UGRD500 - UGRDsfc)**2) + ((VGRD500 - VGRDsfc)**2))
  BS_shallow <- sqrt(((UGRD850 - UGRDsfc)**2) + ((VGRD850 - VGRDsfc)**2))
  avgCAPE <- c(avgCAPE, as.numeric(raster::extract(CAPE, BigDays.sfdfT[i, ], fun = mean)))
  avgsbCAPE <- c(avgsbCAPE, as.numeric(raster::extract(sbCAPE, BigDays.sfdfT[i, ], fun = mean)))
  maxCAPE <- c(maxCAPE, as.numeric(raster::extract(CAPE, BigDays.sfdfT[i, ], fun = max)))
  maxsbCAPE <- c(maxsbCAPE, as.numeric(raster::extract(sbCAPE, BigDays.sfdfT[i, ], fun = max)))
  avgDEW <- c(avgDEW, as.numeric(raster::extract(DEW, BigDays.sfdfT[i, ], fun = mean)))
  maxDEW <- c(maxDEW, as.numeric(raster::extract(DEW, BigDays.sfdfT[i, ], fun = max)))
  avgMR <- c(avgMR, as.numeric(raster::extract(MR, BigDays.sfdfT[i, ], fun = mean)))
  maxMR <- c(maxMR, as.numeric(raster::extract(MR, BigDays.sfdfT[i, ], fun = max)))
  avgHLCY <- c(avgHLCY, as.numeric(raster::extract(HLCY, BigDays.sfdfT[i, ], fun = mean)))
  maxHLCY <- c(maxHLCY, as.numeric(raster::extract(HLCY, BigDays.sfdfT[i, ], fun = max)))
  avgCIN <- c(avgCIN, as.numeric(raster::extract(CIN, BigDays.sfdfT[i, ], fun = mean)))
  avgsbCIN <- c(avgsbCIN, as.numeric(raster::extract(sbCIN, BigDays.sfdfT[i, ], fun = mean)))
  minCIN <- c(minCIN, as.numeric(raster::extract(CIN, BigDays.sfdfT[i, ], fun = min)))
  minsbCIN <- c(minsbCIN, as.numeric(raster::extract(sbCIN, BigDays.sfdfT[i, ], fun = min)))
  avgUSTM <- c(avgUSTM, as.numeric(raster::extract(USTM, BigDays.sfdfT[i, ], fun = mean)))
  maxUSTM <- c(maxUSTM, as.numeric(raster::extract(USTM, BigDays.sfdfT[i, ], fun = max)))
  avgVSTM <- c(avgVSTM, as.numeric(raster::extract(VSTM, BigDays.sfdfT[i, ], fun = mean)))
  maxVSTM <- c(maxVSTM, as.numeric(raster::extract(VSTM, BigDays.sfdfT[i, ], fun = max)))
  avgSM <- c(avgSM, as.numeric(raster::extract(SM, BigDays.sfdfT[i, ], fun = mean)))
  maxSM <- c(maxSM, as.numeric(raster::extract(SM, BigDays.sfdfT[i, ], fun = max)))
  avgRATIO <- c(avgRATIO, as.numeric(raster::extract(RATIO, BigDays.sfdfT[i, ], fun = mean)))
  avgBS_deep <- c(avgBS_deep, as.numeric(raster::extract(BS_deep, BigDays.sfdfT[i, ], fun = mean)))
  maxBS_deep <- c(maxBS_deep, as.numeric(raster::extract(BS_deep, BigDays.sfdfT[i, ], fun = max)))  
  avgBS_shallow <- c(avgBS_shallow, as.numeric(raster::extract(BS_shallow, BigDays.sfdfT[i, ], fun = mean)))
  maxBS_shallow <- c(maxBS_shallow, as.numeric(raster::extract(BS_shallow, BigDays.sfdfT[i, ], fun = max)))
  avgLCL <- c(avgLCL, as.numeric(raster::extract(LCL, BigDays.sfdfT[i,], fun = mean)))
  maxLCL <- c(maxLCL, as.numeric(raster::extract(LCL, BigDays.sfdfT[i,], fun = max)))
  minLCL <- c(minLCL, as.numeric(raster::extract(LCL, BigDays.sfdfT[i,], fun = min)))
}
```

Add environmental data values to the group day means data frame.
```{r, eval = FALSE}
BigDays.sfdfT$avgCAPE <- avgCAPE
BigDays.sfdfT$avgsbCAPE <- avgsbCAPE
BigDays.sfdfT$maxCAPE <- maxCAPE
BigDays.sfdfT$maxsbCAPE <- maxsbCAPE
BigDays.sfdfT$avgDEW <- avgDEW
BigDays.sfdfT$maxDEW <- maxDEW
BigDays.sfdfT$avgMR <- avgMR
BigDays.sfdfT$maxMR <- maxMR
BigDays.sfdfT$avgHLCY <- avgHLCY
BigDays.sfdfT$maxHLCY <- maxHLCY
BigDays.sfdfT$avgCIN <- avgCIN
BigDays.sfdfT$avgsbCIN <- avgsbCIN
BigDays.sfdfT$minCIN <- minCIN
BigDays.sfdfT$minsbCIN <- minsbCIN
BigDays.sfdfT$avgUSTM <- avgUSTM
BigDays.sfdfT$maxUSTM <- maxUSTM
BigDays.sfdfT$avgVSTM <- avgVSTM
BigDays.sfdfT$maxVSTM <- maxVSTM
BigDays.sfdfT$avgBS_deep <- avgBS_deep
BigDays.sfdfT$maxBS_deep <- maxBS_deep
BigDays.sfdfT$avgBS_shallow <- avgBS_shallow
BigDays.sfdfT$maxBS_shallow <- maxBS_shallow
BigDays.sfdfT$avgRATIO <- avgRATIO
BigDays.sfdfT$avgSM <- avgSM
BigDays.sfdfT$maxSM <- maxSM
BigDays.sfdfT$minLCL <- minLCL
BigDays.sfdfT$maxLCL <- maxLCL
BigDays.sfdfT$avgLCL <- avgLCL
```

########################
## Population Density ##
########################

You need to establish a population density for each big day. Round the year to the nearest 10. Example: 1995 should round to 2000 census and 1993 should round to 1990 census. 

Create a function to round to the nearest 5 since we can get population for every 5 years starting 1995 - 2015. 
```{r}
mround <- function(x,base){ 
        base*round(x/base) 
} 
```

Add a new column `PopDensyear` to the dataset. This will round each year to the nearest 5th year. 
```{r}
BigDays.sfdfT <- BigDays.sfdfT %>%
  mutate(PopDensyear = mround(Year, 5))
```

Download the census data: 

Begin with The Gridded Population of the World Volume 3 data (2000). Begin with The Gridded Population of the World Volume 3 data. This data set has population density data available beginning in 1990 and ending in 2000. We are using the 1995 data. 

**1990 - 2000: https://sedac.ciesin.columbia.edu/data/set/gpw-v3-population-density**

Citation: `Center for International Earth Science Information Network - CIESIN - Columbia University, and Centro Internacional de Agricultura Tropical - CIAT. 2005. Gridded Population of the World, Version 3 (GPWv3): Population Density Grid. Palisades, New York: NASA Socioeconomic Data and Applications Center (SEDAC). https://doi.org/10.7927/H4XK8CG2. Accessed DAY MONTH YEAR.`

Population Data for 2000 through 2015 are obtained from the Gridded Population of the World, version four (GPW, v4) from the Socioeconomic Data and Applications Center at Columbia University, USA. The database contain decennial census density estimates for 2000, 2005, 2010, and 2015 represented as people per square kilometer. Densities are based on residential population. 

**2000 - present: https://sedac.ciesin.columbia.edu/data/collection/gpw-v4**

Citation: `Center for International Earth Science Information Network - CIESIN - Columbia University. 2018. Gridded Population of the World, Version 4 (GPWv4): Population Density, Revision 11. Palisades, New York: NASA Socioeconomic Data and Applications Center (SEDAC). https://doi.org/10.7927/H49C6VHW. Accessed DAY MONTH YEAR.`

Load the population raster(s) and crop to defined extent. 
```{r}
#1995 Data: 
  rb <- raster("/Volumes/Work/PopDens/usadens/usads95g/w001001.adf")
  ext = raster::extent(c(-125, -67, 24, 50))
  PopDens = crop(rb, ext)
  PopDens1995 = projectRaster(PopDens, crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
#2000 Data: 
  rb <- raster(paste0("/Volumes/Work/PopDens", "/gpw-v4-population-density-rev11_2000_2pt5_min_tif/gpw_v4_population_density_rev11_2000_2pt5_min.tif"))
  ext = raster::extent(c(-125, -67, 24, 50))
  PopDens = crop(rb, ext)
  PopDens2000 = projectRaster(PopDens, crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
#2005 Data: 
    rb <- raster(paste0("/Volumes/Work/PopDens", "/gpw-v4-population-density-rev11_2005_2pt5_min_tif/gpw_v4_population_density_rev11_2005_2pt5_min.tif"))
  ext = raster::extent(c(-125, -67, 24, 50))
  PopDens = crop(rb, ext)
  PopDens2005 = projectRaster(PopDens, crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
#2010 Data: 
    rb <- raster(paste0("/Volumes/Work/PopDens", "/gpw-v4-population-density-rev11_2010_2pt5_min_tif/gpw_v4_population_density_rev11_2010_2pt5_min.tif"))
  ext = raster::extent(c(-125, -67, 24, 50))
  PopDens = crop(rb, ext)
  PopDens2010 = projectRaster(PopDens, crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
#2015 Data: 
    rb <- raster(paste0("/Volumes/Work/PopDens", "/gpw-v4-population-density-rev11_2015_2pt5_min_tif/gpw_v4_population_density_rev11_2015_2pt5_min.tif"))
  ext = raster::extent(c(-125, -67, 24, 50))
  PopDens = crop(rb, ext)
  PopDens2015 = projectRaster(PopDens, crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
  
  #2020 Data: 
    rb <- raster(paste0("/Volumes/Work/PopDens", "/gpw-v4-population-density-rev11_2020_2pt5_min_tif/gpw_v4_population_density_rev11_2020_2pt5_min.tif"))
  ext = raster::extent(c(-125, -67, 24, 50))
  PopDens = crop(rb, ext)
  PopDens2020 = projectRaster(PopDens, crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
```

Use the `extract` function to obtain the population density.
```{r}
avgPopDens <- numeric()
maxPopDens <- numeric()
totalPopDens <- numeric()

library(raster)
for(i in 1:dim(BigDays.sfdfT)[1]){
  print(i)
  if (BigDays.sfdfT$PopDensyear[i] == 1995) {
  avgPopDens <- c(avgPopDens, as.numeric(raster::extract(PopDens1995, BigDays.sfdfT[i, ], fun = mean, na.rm = TRUE, weights = TRUE, normalizeWeights = FALSE)))
  maxPopDens <- c(maxPopDens, as.numeric(raster::extract(PopDens1995, BigDays.sfdfT[i, ], fun = max, na.rm = TRUE))) 
  totalPopDens <- c(totalPopDens, as.numeric(raster::extract(PopDens1995, BigDays.sfdfT[i, ], fun = sum, na.rm = TRUE))) 
  } else if (BigDays.sfdfT$PopDensyear[i] == 2000){
  avgPopDens <- c(avgPopDens, as.numeric(raster::extract(PopDens2000, BigDays.sfdfT[i,], fun = mean, na.rm = TRUE, weights = TRUE, normalizeWeights = FALSE)))
  maxPopDens <- c(maxPopDens, as.numeric(raster::extract(PopDens2000, BigDays.sfdfT[i,], fun = max, na.rm = TRUE)))
  totalPopDens <- c(totalPopDens, as.numeric(raster::extract(PopDens1995, BigDays.sfdfT[i, ], fun = sum, na.rm = TRUE))) 
  } else if (BigDays.sfdfT$PopDensyear[i] == 2005){
  avgPopDens <- c(avgPopDens, as.numeric(raster::extract(PopDens2005, BigDays.sfdfT[i,], fun = mean, na.rm = TRUE, weights = TRUE, normalizeWeights = FALSE)))
  maxPopDens <- c(maxPopDens, as.numeric(raster::extract(PopDens2005, BigDays.sfdfT[i,], fun = max, na.rm = TRUE)))
  totalPopDens <- c(totalPopDens, as.numeric(raster::extract(PopDens1995, BigDays.sfdfT[i, ], fun = sum, na.rm = TRUE))) 
  } else if (BigDays.sfdfT$PopDensyear[i] == 2010){
  avgPopDens <- c(avgPopDens, as.numeric(raster::extract(PopDens2010, BigDays.sfdfT[i,], fun = mean, na.rm = TRUE, weights = TRUE, normalizeWeights = FALSE)))
  maxPopDens <- c(maxPopDens, as.numeric(raster::extract(PopDens2010, BigDays.sfdfT[i,], fun = max, na.rm = TRUE)))
  totalPopDens <- c(totalPopDens, as.numeric(raster::extract(PopDens1995, BigDays.sfdfT[i, ], fun = sum, na.rm = TRUE))) 
  } else if (BigDays.sfdfT$PopDensyear[i] >= 2015){
  avgPopDens <- c(avgPopDens, as.numeric(raster::extract(PopDens2015, BigDays.sfdfT[i,], fun = mean, na.rm = TRUE, weights = TRUE, normalizeWeights = FALSE)))
  maxPopDens <- c(maxPopDens, as.numeric(raster::extract(PopDens2015, BigDays.sfdfT[i,], fun = max, na.rm = TRUE)))
  totalPopDens <- c(totalPopDens, as.numeric(raster::extract(PopDens1995, BigDays.sfdfT[i, ], fun = sum, na.rm = TRUE))) 
  }
}
```
 
Add the max and avg population density values to the Big Day data: 
```{r}
BigDays.sfdfT <- cbind(BigDays.sfdfT, avgPopDens) 
BigDays.sfdfT <- cbind(BigDays.sfdfT, maxPopDens) 
BigDays.sfdfT <- cbind(BigDays.sfdfT, totalPopDens) 
```


```{r}
#save(BigDays.sfdfT, BigDayTornadoes, BigDayCentroids.df, Groups.sfdfT, GroupTornadoes, All_Tornadoes, file = "TornadoOutbreaks.RData")
```

*Get the latitude and longitude for each big day centroid*
```{r}
BigDayCentroids.df <- st_centroid(BigDays.sfdfT)
test <- as_Spatial(BigDayCentroids.df)
library(rgdal)
spgeo <- spTransform(test, CRS("+proj=longlat +datum=WGS84"))
test <- st_as_sf(spgeo)
coords <- as.data.frame(st_coordinates(test))
colnames(coords)[colnames(coords)=="X"] <- "Lon"
colnames(coords)[colnames(coords)=="Y"] <- "Lat"

BigDays.sfdfT <- cbind(as.data.frame(BigDays.sfdfT), as.data.frame(coords))
```

*Get the total population from the cities that fall within each big day*
```{r}
library(USAboundaries)
cities <- us_cities()
BigDays.sfdfT <- st_as_sf(BigDays.sfdfT)

BigDays.sfdfT <- st_convex_hull(BigDays.sfdfT)
BigDays.sfdfT <- st_transform(BigDays.sfdfT, 
  crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
cities <- st_transform(cities, 
  crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")

totalPOP <- numeric()

for(i in 1:dim(BigDays.sfdfT)[1]){
    popinhull <- cities[BigDays.sfdfT[i,],]
    dayPOP <- sum(popinhull$population)
    totalPOP <- append(totalPOP, dayPOP)
}
BigDays.sfdfT <- cbind(BigDays.sfdfT, totalPOP)
```
Save the data. 

```{r}
#save(BigDays.sfdfT, BigDayTornadoes, BigDayCentroids.df, Groups.sfdfT, GroupTornadoes, All_Tornadoes, file = "TornadoOutbreaks.RData")
#load("TornadoOutbreaks.RData")
```

```{r, eval = FALSE}
updat <- BigDays.sfdfT
load("TornadoOutbreaks.RData")
NARRdat <- BigDays.sfdfT[,33:67]
NARRdat <- st_drop_geometry(NARRdat)

BigDays.sfdfT <- cbind(updat,NARRdat)
```

Greenhouse Gas Emission Data: 

Annual: 
Carbon Dioxide: 1979
Methane: 1979
Nitrous Oxide: 1979
Sulfur Hexaflouride: 1979
Ozone: 1979
AGGI: 1979

```{r}
Annual_GHG <- read.csv(file = "AnnualAverages_GHG.csv")

colnames(Annual_GHG)[1] <- "Year"
```

```{r}
Annual_GHG <- Annual_GHG %>%
  filter(Year >= 1984) %>%
  dplyr::select(Year, CO2_ppm, CH4_ppb, N2O_ppb, SF6_ppt, O3_DU, AGGI)
```


10-year average prior: 

```{r}
Year= c()
CO2_10year = c()
CH4_10year = c()
N2O_10year = c()
SF6_10year = c()
O3_10year = c()
AGGI_10year = c()

for(h in 1:dim(BigDays.sfdfT)[1]){
  i = BigDays.sfdfT$Year[h]
  startyear = i - 10
  endyear = i - 1
  startrow = which(Annual_GHG$Year == startyear)
  endrow = which(Annual_GHG$Year == endyear)
  Year = c(Year, i)
  CO2_10year = c(CO2_10year, mean(Annual_GHG$CO2_ppm[startrow:endrow]))
  CH4_10year = c(CH4_10year, mean(Annual_GHG$CH4_ppb[startrow:endrow]))
  N2O_10year = c(N2O_10year, mean(Annual_GHG$N2O_ppb[startrow:endrow]))
  SF6_10year = c(SF6_10year, mean(Annual_GHG$SF6_ppt[startrow:endrow]))
  O3_10year = c(O3_10year, mean(Annual_GHG$O3_DU[startrow:endrow]))
  AGGI_10year = c(AGGI_10year, mean(Annual_GHG$AGGI[startrow:endrow]))
}

GHG_10year <- cbind(Year, CO2_10year, CH4_10year, N2O_10year, SF6_10year, O3_10year, AGGI_10year)
```
5-year averages: 
```{r}
Year= c()
CO2_5year = c()
CH4_5year = c()
N2O_5year = c()
SF6_5year = c()
O3_5year = c()
AGGI_5year = c()

for(h in 1:dim(BigDays.sfdfT)[1]){
  i = BigDays.sfdfT$Year[h]
startyear = i - 5
endyear = i - 1
startrow = which(Annual_GHG$Year == startyear)
endrow = which(Annual_GHG$Year == endyear)
Year = c(Year, i)
CO2_5year = c(CO2_5year, mean(Annual_GHG$CO2_ppm[startrow:endrow]))
CH4_5year = c(CH4_5year, mean(Annual_GHG$CH4_ppb[startrow:endrow]))
  N2O_5year = c(N2O_5year, mean(Annual_GHG$N2O_ppb[startrow:endrow]))
  SF6_5year = c(SF6_5year, mean(Annual_GHG$SF6_ppt[startrow:endrow]))
O3_5year = c(O3_5year, mean(Annual_GHG$O3_DU[startrow:endrow]))
AGGI_5year = c(AGGI_5year, mean(Annual_GHG$AGGI[startrow:endrow]))
}

GHG_5year <- cbind(Year, CO2_5year, CH4_5year, N2O_5year, SF6_5year, O3_5year, AGGI_5year)
```

```{r}
Year= c()
CO2_1year = c()
CH4_1year = c()
N2O_1year = c()
SF6_1year = c()
O3_1year = c()
AGGI_1year = c()

for(h in 1:dim(BigDays.sfdfT)[1]){
  i = BigDays.sfdfT$Year[h]
startyear = i - 1
startrow = which(Annual_GHG$Year == startyear)
Year = c(Year, i)
CO2_1year = c(CO2_1year, (Annual_GHG$CO2_ppm[startrow]))
CH4_1year = c(CH4_1year, (Annual_GHG$CH4_ppb[startrow]))
  N2O_1year = c(N2O_1year, mean(Annual_GHG$N2O_ppb[startrow:endrow]))
  SF6_1year = c(SF6_1year, mean(Annual_GHG$SF6_ppt[startrow:endrow]))
O3_1year = c(O3_1year, (Annual_GHG$O3_DU[startrow]))
AGGI_1year = c(AGGI_1year, (Annual_GHG$AGGI[startrow]))
}

GHG_1year <- cbind(Year, CO2_1year, CH4_1year, N2O_1year, SF6_1year,O3_1year, AGGI_1year)
```


```{r}
BigDays.sfdfT <- cbind(BigDays.sfdfT, GHG_10year[,2:7], GHG_5year[,2:7], GHG_1year[,2:7])
```

```{r}
BigDays.sfdfT <- st_as_sf(BigDays.sfdfT)
BigDayTornadoes <- st_as_sf(BigDayTornadoes)
BigDayCentroids.df <- st_as_sf(BigDayCentroids.df)
Groups.sfdfT <- st_as_sf(Groups.sfdfT)
GroupTornadoes <- st_as_sf(GroupTornadoes)
All_Tornadoes <- st_as_sf(All_Tornadoes)
#save(BigDays.sfdfT, BigDayTornadoes, BigDayCentroids.df, Groups.sfdfT, GroupTornadoes, All_Tornadoes, file = "TornadoOutbreaks.RData")
#load("TornadoOutbreaks.RData")
```

Monthly: 
Carbon Dioxide: 1958
Methane: 1983
Nitrous Oxide: 2001
Sulfur Hexaflouride: 1997

Daily: 
Carbon Dioxide: 1974

#################
## Climate Data: 
#################

```{r, eval = FALSE}
load("BigDays.RData")
```

Surface Temperatures from National Oceanic and Atmospheric Association's (NOAA) Global Surface Temperature (NOAAGlobalTemp) data provided by the NOAA/OAR/ESRL PSL (\url{https://psl.noaa.gov/data/gridded/data.noaaglobaltemp.html}). NOAAGlobalTemp is a combination of land and ocean temperatures combined into a singular dataset. The data includes monthly surface temperatures (both land and water) from 1880 to present. It has a spatial resolution of 5$^\circ$ by 5$^\circ$. 

In climate change studies, temperature anomalies are more important than absolute temperature. A temperature anomaly is the difference from an average, or baseline, temperature. The baseline temperature is typically computed by averaging 30 or more years of temperature data. A positive anomaly indicates the observed temperature was warmer than the baseline, while a negative anomaly indicates the observed temperature was cooler than the baseline. When calculating an average of absolute temperatures, things like station location or elevation will have an effect on the data (ex. higher elevations tend to be cooler than lower elevations and urban areas tend to be warmer than rural areas). However, when looking at anomalies, those factors are less critical. For example, a summer month over an area may be cooler than average, both at a mountain top and in a nearby valley, but the absolute temperatures will be quite different at the two locations.

Using anomalies also helps minimize problems when stations are added, removed, or missing from the monitoring network. The above diagram shows absolute temperatures (lines) for five neighboring stations, with the 2008 anomalies as symbols. Notice how all of the anomalies fit into a tiny range when compared to the absolute temperatures. Even if one station were removed from the record, the average anomaly would not change significantly, but the overall average temperature could change significantly depending on which station dropped out of the record. For example, if the coolest station (Mt. Mitchell) were removed from the record, the average absolute temperature would become significantly warmer. However, because its anomaly is similar to the neighboring stations, the average anomaly would change much less.

```{r}
#install.packages("terra")
library(raster)
ncpath <- "/Users/zoesearcy/Desktop/ResearchProjects/TornEnvironmentsandGHGs/"
ncname <- "air.mon.anom"  
ncfname <- paste(ncpath, ncname, ".nc", sep="")
library(ncdf4)
tmp_raster <- terra::rast(ncfname, subds ="air")
class(tmp_raster)

January1994 <- subset(x = tmp_raster, 
                      subset = 1369)
plot(January1994)
```

Transform the BigDays.sfdfT to the same projection as the raster data. Check the extent of BigDays.sfdfT to ensure the geometry is in degrees latitude and longitude. Rotate the `January1994` raster to get longitude in -180 to 180 degrees longitude (currently 0 to 360 degrees longitude).
```{r,eval = FALSE}
BigDays.sfdfT <- st_transform(BigDays.sfdfT, CRS("+proj=longlat +datum=WGS84"))

extent(BigDays.sfdfT)

January1994 <- rotate(January1994)
```

extent(BigDays.sfdfT)
class      : Extent 
xmin       : -116.93 
xmax       : -70.57 
ymin       : 24.9879 
ymax       : 49 

January1994
class      : RasterLayer 
dimensions : 36, 72, 2592  (nrow, ncol, ncell)
resolution : 5, 5  (x, y)
extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
crs        : +proj=longlat +datum=WGS84 +no_defs 
source     : memory
names      : X1994.01.01 
values     : -8.546114, 5.79  (min, max)
z-value    : 1994-01-01 

#How do you set extent boundaries? Is it easier to manually set them? 
Create a spatial extent for the entire contiguous United States. You do this by creating a Spatial Polygons data frame then converting to an sf object. 

```{r,eval = FALSE}
USextent <- as(raster::extent(-130, -65, 25, 49), "SpatialPolygons") %>%
  st_as_sf()
```

Crop the January 1994 data to the extent of the United States. 
```{r,eval = FALSE}
Jan94 <- crop(January1994, extent(USextent))
Jan94_extent <- mask(Jan94, USextent)

plot(Jan94_extent)
```

```{r}
suppressMessages(library(sf))
suppressMessages(library(dplyr))
suppressMessages(library(lubridate))
suppressMessages(library(lutz))
suppressMessages(library(xts))
suppressMessages(library(chron))
suppressMessages(library(sp))
suppressMessages(library(USAboundaries))
suppressMessages(library(tmap))
suppressMessages(library(raster))
suppressMessages(library(ggplot2))
suppressMessages(library(gridExtra))
sts <- state.name[!state.name %in% c("Alaska", "Hawaii")]
stateBorders <- us_states(states = sts)
```

Plot a map of the January 1994 air temperature data over the contiguous United States. 
```{r,eval = FALSE}
library(tmap)
tm_shape(Jan94_extent, 
         projection = merc, 
         is.master = TRUE, line.center.type = "midpoint") +
  tm_raster(title = "", midpoint = NA) +
  tm_format(title = expression(paste("January 1994 surface air temperature")),
                    "World", 
                    legend.position = c("right", "bottom"),
                    attr.position = c("right", "bottom"),
                    legend.frame = FALSE,
                    inner.margins = c(0, 0.01, 0, .01), #bottom, left, top
                    legend.text.size = 1, 
                    legend.width = -0.28, 
                    legend.title.size=1, 
                    legend.bg.color = "white", 
                    title.size = 1.4) +
#tm_shape(counties) +
#  tm_borders(col = "grey") +
  tm_scale_bar(width = 0.3, size = 1, position = c("right", "top"), color.dark = "gray70") +
tm_shape(stateBorders, midpoint = NA) +
  tm_borders() 
```

```{r, eval = FALSE}
min_airtemp <- numeric()
avg_airtemp <- numeric()
max_airtemp <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  Year_add <- (BigDays.sfdfT$Year[i] - min(BigDays.sfdfT$Year)) * 12
  Month_add <- BigDays.sfdfT$Mo[i]
  total <- Year_add + Month_add
  val <- (1369 + total)
  ras <- subset(tmp_raster, val)
  #ras <- rotate(ras)
 # ras <- crop(ras, extent(USextent))
 # ras <- mask(ras, USextent)
  min_airtemp <- c(min_airtemp, min(terra::values(ras), na.rm = TRUE))
  avg_airtemp <- c(avg_airtemp, mean(terra::values(ras), na.rm = TRUE))
  max_airtemp <- c(max_airtemp, max(terra::values(ras), na.rm = TRUE))
}
```

```{r, eval = FALSE}
BigDays.sfdfT$min_airtemp <- min_airtemp
BigDays.sfdfT$max_airtemp <- max_airtemp
BigDays.sfdfT$avg_airtemp <- avg_airtemp

names(BigDays.sfdfT)[names(BigDays.sfdfT) == 'min_airtemp'] <- 'min_globalairtemp'
names(BigDays.sfdfT)[names(BigDays.sfdfT) == 'avg_airtemp'] <- 'avg_globalairtemp'
names(BigDays.sfdfT)[names(BigDays.sfdfT) == 'max_airtemp'] <- 'max_globalairtemp'
```

Save the data.
```{r, eval = FALSE}
#save(All_Tornadoes, BigDayCentroids.df, BigDays.sfdfT, BigDayTornadoes, Groups.sfdfT, file = "ClusterData.RData")
#save(All_Tornadoes, BigDayCentroids.df, BigDays.sfdfT, BigDayTornadoes, Groups.sfdfT, file = "ClusterData_Backup.RData")
```


## Monthly Climate Data ##


PROSPECTUS: 15$^{\circ}$N to 25$^{\circ}$N and 70$^{\circ}$W to 90$^{\circ}$W

```{r, eval = FALSE}
#download.file(url = "https://psl.noaa.gov/cgi-bin/data/timeseries/timeseries.pl?ntype=1&var=SST&level=2000&lat1=15&lat2=25&lon1=270&lon2=290+&iseas=0&mon1=0&mon2=0&iarea=0&typeout=1&Submit=Create+Timeseries" ,
              #destfile = "Atlantic_SST.csv", mode = "wb")

Atl_SST <- read.csv(file = "Atlantic_SST.csv")

```

Get the sea surface temperature for each big day. 
```{r, eval = FALSE}
Atlantic_SST <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  month_column = as.numeric(BigDays.sfdfT$Month[i]) + 1
  year_row = BigDays.sfdfT$Year[i] - 1994
  Atlantic_SST <- c(Atlantic_SST, Atl_SST[47+ year_row, month_column]) #47 is year 1994 in the Atlantic_SST data frame
}  
```

Add sea surface temperatures to the BigDays.sfdfT file. 
```{r, eval = FALSE}
BigDays.sfdfT$Atlantic_SST <- Atlantic_SST
names(BigDays.sfdfT)[names(BigDays.sfdfT) == 'Atlantic_SST'] <- 'monthlyATLSST'
```


##Alaska SST 
60.0 to 50.5 N and 133.1 to 157.5 W
```{r, eval = FALSE}
#download.file(url = "https://psl.noaa.gov/cgi-bin/data/timeseries/timeseries.pl?ntype=1&var=SST&level=2000&lat1=50.36&lat2=59.95&lon1=133.08&lon2=156.66+&iseas=0&mon1=0&mon2=0&iarea=0&typeout=1&Submit=Create+Timeseries", destfile = "Alaska_SST.csv", mode = "wb")

Alaska_SST <- read.csv(file = "Alaska_SST.csv")
```

Get the sea surface temperature for each big day. 
```{r, eval = FALSE}
GAK_SST <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  month_column = as.numeric(BigDays.sfdfT$Month[i]) + 1
  year_row = BigDays.sfdfT$Year[i] - 1994
  GAK_SST <- c(GAK_SST, Alaska_SST[47+ year_row, month_column]) #47 is year 1994 in the Atlantic_SST data frame when removing header
}  
```

Add sea surface temperatures to the BigDays.sfdfT file. 
```{r, eval = FALSE}
BigDays.sfdfT$GAK_SST <- GAK_SST
names(BigDays.sfdfT)[names(BigDays.sfdfT) == 'GAK_SST'] <- 'monthlyGAKSST'
```

## Nino 3.4 ##
-99.99							
NINA34							
5N-5S 17	0W-120W						
HadISST							
https://	psl.noaa	.gov/g	cos_wgsp	/Timeser	ies/Nino	34/	

PROSPECTUS: 120$^{\circ}$W to 170$^{\circ}$W and 5$^{\circ}$N to 5$^{\circ}$S.

```{r, eval = FALSE}
#download.file(url = "https://psl.noaa.gov/gcos_wgsp/Timeseries/Data/nino34.long.data",
              destfile = "Nino3_4.csv", mode = "wb")

NINO <- read.csv(file = "Nino3_4.csv")
```

```{r, eval = FALSE}
ElNino34 <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  month_column = as.numeric(BigDays.sfdfT$Month[i]) + 1
  year_row = BigDays.sfdfT$Year[i] - 1994
  ElNino34 <- c(ElNino34, NINO[125+ year_row, month_column]) #125 is year 1994 in the NINO data frame
}  
```

Add El Nino 3.4 data to the BigDays.sfdfT file. 
```{r, eval = FALSE}
BigDays.sfdfT$ElNino34 <- ElNino34
names(BigDays.sfdfT)[names(BigDays.sfdfT) == 'ElNino34'] <- 'monthlyElNino34'
```

Save the data.
```{r, eval = FALSE}
#save(All_Tornadoes, BigDayCentroids.df, BigDays.sfdfT, BigDayTornadoes, Groups.sfdfT, file = "ClusterData.RData")
#save(All_Tornadoes, BigDayCentroids.df, BigDays.sfdfT, BigDayTornadoes, Groups.sfdfT, file = "ClusterData_Backup.RData")
```

Arctic Sea Ice

Comes from the Sea Ice Index, Sea Ice Concentrations from Nimbus-7 SMMR SSMIS and DMSP SSM/I Passive Microwave Data is the input data that the Sea Ice Index is derived from. Extent and Area are in millions of square kilometers. N 90 to 30.98 E: 180 to -180 https://nsidc.org/data/g02135?qt-data_set_tabs=2#qt-data_set_tabs

```{r, eval = FALSE}
seaice <- read.csv(file = "SeaIce.csv")
```

```{r, eval = FALSE}

SIE <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  month_column = as.numeric(BigDays.sfdfT$Month[i]) + 1
  year_row = BigDays.sfdfT$Year[i] - 1994
  SIE <- c(SIE, seaice[17+ year_row, month_column]) #125 is year 1994 in the NINO data frame
}  
```

Add Sea Ice Extent to the BigDays.sfdfT file. 
```{r, eval = FALSE}
BigDays.sfdfT$SIE<- SIE

names(BigDays.sfdfT)[names(BigDays.sfdfT) == 'SIE'] <- 'monthlySIE'
```

Save the data.
```{r, eval = FALSE}
#save(All_Tornadoes, BigDayCentroids.df, BigDays.sfdfT, BigDayTornadoes, Groups.sfdfT, file = "ClusterData.RData")
#save(All_Tornadoes, BigDayCentroids.df, BigDays.sfdfT, BigDayTornadoes, Groups.sfdfT, file = "ClusterData_Backup.RData")
```

##Daily Climate Data: 
NAO:
The index is calculated with the easiest method which is the difference in the atmospheric pressure of a meteorological station near the Azores (Lisbon and Ponta Delgada or or Gibraltar, etc.) and a station near the depression of Iceland (Reykjavik and Akureyri or Stykkisholmur, etc.) The positive NAO phase represents a stronger than usual difference in pressure between the two regions. Winds from the west dominate, bringing with them warm air, while the position of the jet stream enables stronger and more frequent storms to travel across the Atlantic.

https://psl.noaa.gov/data/timeseries/daily/NAO/
Calculation Method: The indices are based on centers-of-action of 500mb height patterns. These time series utilize the NCEP/NCAR R1 dataset. The area averaged region 55-70N;70W-10W is subtracted from 35-45N; 70W-10W. The 1981-2010 period was used as climatology. Before computing these indices the 500mb height fields are spectrally truncated to total wavenumber 10 in order to emphasize large-scale aspects of the teleconnections.

*Daily NAO: *
```{r, eval = FALSE}

#download.file(url = "ftp://ftp.cdc.noaa.gov/Public/map/teleconnections/nao.reanalysis.t10trunc.1948-present.txt" , destfile = "DailyNAO.csv", mode = "wb")

NAO_daily<- read.csv("DailyNAO.csv")
colnames(NAO_daily) = c("Year", "Mo", "Day", "Value")

max(NAO_daily$Value)

NAO_daily$StandardizedNAO <- NAO_daily$Value/100

NAO_daily <- NAO_daily %>%
  filter(Year >= 1994 & Year <= 2022)

NAO_daily$Date <- as.Date(with(NAO_daily, paste(Year, Mo, Day, sep="-")), "%Y-%m-%d")
```

```{r, eval = FALSE}
NAO_dailyvals <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  row = NAO_daily %>%
    filter(Date == BigDays.sfdfT$cDate[i])
    NAO_dailyvals <- c(NAO_dailyvals, row$StandardizedNAO)
}

BigDays.sfdfT$NAO_daily <- NAO_dailyvals
```

*Get the 5 day average NAO*
```{r, eval = FALSE}
NAO_5day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(NAO_daily$Date == BigDays.sfdfT$cDate[i])
  startrow = rownum-5
  dat <- NAO_daily[startrow:rownum,]
  NAO_5day <- c(NAO_5day, mean(dat$StandardizedNAO))
} 

BigDays.sfdfT$NAO_5day <- NAO_5day

```

*Get the 10 day average NAO*
```{r, eval = FALSE}
NAO_10day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(NAO_daily$Date == BigDays.sfdfT$cDate[i])
  startrow = rownum-10
  dat <- NAO_daily[startrow:rownum,]
  NAO_10day <- c(NAO_10day, mean(dat$StandardizedNAO))
} 

BigDays.sfdfT$NAO_10day <- NAO_10day

```
*Get the 15 day average NAO*
```{r, eval = FALSE}
NAO_15day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(NAO_daily$Date == BigDays.sfdfT$cDate[i])
  startrow = rownum-15
  dat <- NAO_daily[startrow:rownum,]
  NAO_15day <- c(NAO_15day, mean(dat$StandardizedNAO))
} 

BigDays.sfdfT$NAO_15day <- NAO_15day

```
*Get the 20 day average NAO*
```{r, eval = FALSE}
NAO_20day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(NAO_daily$Date == BigDays.sfdfT$cDate[i])
  startrow = rownum-20
  dat <- NAO_daily[startrow:rownum,]
  NAO_20day <- c(NAO_20day, mean(dat$StandardizedNAO))
} 

BigDays.sfdfT$NAO_20day <- NAO_20day

```

*Get the 30 day average NAO*
```{r, eval = FALSE}
NAO_30day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(NAO_daily$Date == BigDays.sfdfT$cDate[i])
  startrow = rownum-30
  dat <- NAO_daily[startrow:rownum,]
  NAO_30day <- c(NAO_30day, mean(dat$StandardizedNAO))
} 

BigDays.sfdfT$NAO_30day <- NAO_30day

```

*Daily MJO: *

```{r, eval = FALSE}
#download.file(url = "http://www.bom.gov.au/climate/mjo/graphics/rmm.74toRealtime.txt", destfile = "DailyMJO.csv", mode = "wb")
MJO_daily<- read.csv("DailyMJO.csv")
MJO_daily <- MJO_daily %>%
  filter(year >= 1994 & year <= 2022)

MJO_daily$Date <- as.Date(with(MJO_daily, paste(year, month, day, sep="-")), "%Y-%m-%d")
```

```{r, eval = FALSE}
MJO_dailyvals <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  row = MJO_daily %>%
    filter(Date == BigDays.sfdfT$cDate[i])
    MJO_dailyvals <- c(MJO_dailyvals, row$amplitude)
}

BigDays.sfdfT$MJO_daily <- MJO_dailyvals
```

*Get the 5 day average MJO*
```{r, eval = FALSE}
MJO_5day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(MJO_daily$Date == BigDays.sfdfT$cDate[i])
  startrow = rownum-5
  dat <- MJO_daily[startrow:rownum,]
  MJO_5day <- c(MJO_5day, mean(dat$amplitude))
} 

BigDays.sfdfT$MJO_5day <- MJO_5day

```

*Get the 10 day average MJO*
```{r, eval = FALSE}
MJO_10day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(MJO_daily$Date == BigDays.sfdfT$cDate[i])
  startrow = rownum-10
  dat <- MJO_daily[startrow:rownum,]
  MJO_10day <- c(MJO_10day, mean(dat$amplitude))
} 

BigDays.sfdfT$MJO_10day <- MJO_10day

```
*Get the 15 day average MJO*
```{r, eval = FALSE}
MJO_15day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(MJO_daily$Date == BigDays.sfdfT$cDate[i])
  startrow = rownum-15
  dat <- MJO_daily[startrow:rownum,]
  MJO_15day <- c(MJO_15day, mean(dat$amplitude))
} 

BigDays.sfdfT$MJO_15day <- MJO_15day

```
*Get the 20 day average MJO*
```{r, eval = FALSE}
MJO_20day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(MJO_daily$Date == BigDays.sfdfT$cDate[i])
  startrow = rownum-20
  dat <- MJO_daily[startrow:rownum,]
  MJO_20day <- c(MJO_20day, mean(dat$amplitude))
} 

BigDays.sfdfT$MJO_20day <- MJO_20day

```

*Get the 30 day average MJO*
```{r, eval = FALSE}
MJO_30day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(MJO_daily$Date == BigDays.sfdfT$cDate[i])
  startrow = rownum-30
  dat <- MJO_daily[startrow:rownum,]
  MJO_30day <- c(MJO_30day, mean(dat$amplitude))
} 

BigDays.sfdfT$MJO_30day <- MJO_30day

```


PNA:  https://psl.noaa.gov/data/timeseries/daily/PNA/

The indices are based on centers-of-action of 500mb height patterns. These time series utilize the NCEP/NCAR R1 dataset. The region is [(15-25N, 180-140W)-(40-50N, 180-140W)+(45-60N, 125W-105W)-(25-35N, 90W-70W)].

The Pacific/ North American teleconnection pattern (PNA) is one of the most prominent modes of low-frequency variability in the Northern Hemisphere extratropics. The positive phase of the PNA pattern features above-average heights in the vicinity of Hawaii and over the intermountain region of North America, and below-average heights located south of the Aleutian Islands and over the southeastern United States. The PNA pattern is associated with strong fluctuations in the strength and location of the East Asian jet stream. The positive phase is associated with an enhanced East Asian jet stream and with an eastward shift in the jet exit region toward the western United States. The negative phase is associated with a westward retraction of that jet stream toward eastern Asia, blocking activity over the high latitudes of the North pacific, and a strong split-flow configuration over the central North Pacific.

The positive phase of the PNA pattern is associated with above-average temperatures over western Canada and the extreme western United States, and below-average temperatures across the south-central and southeastern U.S. The PNA tends to have little impact on surface temperature variability over North America during summer. The associated precipitation anomalies include above-average totals in the Gulf of Alaska extending into the Pacific Northwestern United States, and below-average totals over the upper Midwestern United States.

*Daily PNA:*

```{r, eval = FALSE}
PNA_daily<- read.csv("DailyPNA.csv")
colnames(PNA_daily) = c("Year", "Mo", "Day", "Value")
max(PNA_daily$Value) #These need to be divided by 100

PNA_daily$StandardizedPNA <- PNA_daily$Value/100
PNA_daily <- PNA_daily %>%
  filter(Year >= 1994 & Year <= 2022)

PNA_daily$Date <- as.Date(with(PNA_daily, paste(Year, Mo, Day, sep="-")), "%Y-%m-%d")
```

```{r, eval = FALSE}
PNA_dailyvals <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  row = PNA_daily %>%
    filter(Date == BigDays.sfdfT$cDate[i])
    PNA_dailyvals <- c(PNA_dailyvals, row$StandardizedPNA)
}

BigDays.sfdfT$PNA_daily <- PNA_dailyvals
```

*Get the 5 day average PNA*
```{r, eval = FALSE}
PNA_5day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(PNA_daily$Date == BigDays.sfdfT$cDate[i])
  startrow = rownum-5
  dat <- PNA_daily[startrow:rownum,]
  PNA_5day <- c(PNA_5day, mean(dat$StandardizedPNA))
} 

BigDays.sfdfT$PNA_5day <- PNA_5day

```

*Get the 10 day average PNA*
```{r, eval = FALSE}
PNA_10day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(PNA_daily$Date == BigDays.sfdfT$cDate[i])
  startrow = rownum-10
  dat <- PNA_daily[startrow:rownum,]
  PNA_10day <- c(PNA_10day, mean(dat$StandardizedPNA))
} 

BigDays.sfdfT$PNA_10day <- PNA_10day

```

```{r, eval = FALSE}
PNA_15day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(PNA_daily$Date == BigDays.sfdfT$cDate[i])
  startrow = rownum-15
  dat <- PNA_daily[startrow:rownum,]
  PNA_15day <- c(PNA_15day, mean(dat$StandardizedPNA))
} 

BigDays.sfdfT$PNA_15day <- PNA_15day

```

*Get the 20 day average PNA*
```{r, eval = FALSE}
PNA_20day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(PNA_daily$Date == BigDays.sfdfT$cDate[i])
  startrow = rownum-20
  dat <- PNA_daily[startrow:rownum,]
  PNA_20day <- c(PNA_20day, mean(dat$StandardizedPNA))
} 

BigDays.sfdfT$PNA_20day <- PNA_20day

```

*Get the 30 day average PNA*
```{r, eval = FALSE}
PNA_30day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(PNA_daily$Date == BigDays.sfdfT$cDate[i])
  startrow = rownum-30
  dat <- PNA_daily[startrow:rownum,]
  PNA_30day <- c(PNA_30day, mean(dat$StandardizedPNA))
} 

BigDays.sfdfT$PNA_30day <- PNA_30day

```


Save the data.
```{r, eval = FALSE}
#save(All_Tornadoes, BigDayCentroids.df, BigDays.sfdfT, BigDayTornadoes, Groups.sfdfT, file = "ClusterData.RData")
#save(All_Tornadoes, BigDayCentroids.df, BigDays.sfdfT, BigDayTornadoes, Groups.sfdfT, file = "ClusterData_Backup.RData")
```

```{r}
load("ClusterData.RData")
```






*Daily SST: *
Data from: https://psl.noaa.gov/cgi-bin/db_search/DBSearch.pl?Dataset=NOAA+High-resolution+Blended+Analysis&Variable=Sea+Surface+Temperature&group=0&submit=Search
```{r, eval = FALSE}
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.1994.nc", destfile= "SST1994.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.1995.nc", destfile= "SST1995.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.1996.nc", destfile= "SST1996.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.1997.nc", destfile= "SST1997.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.1998.nc", destfile= "SST1998.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.1999.nc", destfile= "SST1999.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2000.nc", destfile= "SST2000.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2001.nc", destfile= "SST12001.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2002.nc", destfile= "SST2002.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2003.nc", destfile= "SST2003.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2004.nc", destfile= "SST2004.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2005.nc", destfile= "SST2005.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2006.nc", destfile= "SST2006.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2008.nc", destfile= "SST2008.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2009.nc", destfile= "SST2009.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2007.nc", destfile= "SST2007.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2010.nc", destfile= "SST2010.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2011.nc", destfile= "SST2011.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2012.nc", destfile= "SST2012.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2013.nc", destfile= "SST2013.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2014.nc", destfile= "SST2014.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2015.nc", destfile= "SST2015.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2016.nc", destfile= "SST2016.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2017.nc", destfile= "SST2017.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2018.nc", destfile= "SST2018.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2019.nc", destfile= "SST2019.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2020.nc", destfile= "SST2020.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2021.nc", destfile= "SST2021.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2022.nc", destfile= "SST2022.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.2023.nc", destfile= "SST2023.nc", mode = "wb")
```

```{r, eval = FALSE}
library(raster)
library(ncdf4)
ncpath <- "E:/Zoe/Projects/tornenvir_climate/"
ncname <- "SST1994"  
ncfname <- paste(ncpath, ncname, ".nc", sep="")
library(ncdf4)
tmp_raster <- terra::rast(ncfname)
class(tmp_raster)
```


```{r, eval = FALSE}
year <- seq(1994,2022, 1)

leap <- seq(1996,2022, 4)
leapyear <- sort(rep(seq(1996,2022, 4), 366))
leapday <- rep(seq(1,366,1), length(leap))


nonleap = year[!year %in% leap]
nonleapyear <- sort(rep(nonleap, 365))
nonleapday <- rep(seq(1,365,1), length(nonleap))
nonleap <- as.data.frame(cbind(nonleapyear, nonleapday))


leap <- as.data.frame(cbind(leapyear, leapday))
colnames(leap) <- c("Year", "Day")

nonleap <- as.data.frame(cbind(nonleapyear, nonleapday))
colnames(nonleap) <- c("Year", "Day")


ClimVars <-rbind(leap, nonleap)
ClimVars <- ClimVars[order(ClimVars$Year),]
```

```{r, eval = FALSE}
#library(terra)
avg_CSST <- numeric()
avg_GAKSST <- numeric()
avg_Nino34 <- numeric()
max_CSST <- numeric()
max_GAKSST <- numeric()
max_Nino34 <- numeric()
min_CSST <- numeric()
min_GAKSST <- numeric()
min_Nino34 <- numeric()

for(i in 2019:2022) {
  ncpath <- "/Users/zoesearcy/Desktop/ResearchProjects/TornEnvironmentsandGHGs/"
  ncname <- paste0("SST", i)  
  ncfname <- paste(ncpath, ncname, ".nc", sep="")
  raster_dat <- terra::rast(ncfname)
  print(i)
    for (j in 1:dim(raster_dat)[3]) {
      print(j)
      sub <- subset(raster_dat, j)
      sub <- rotate(sub)

      Caribbean <- c(-90, -70, 15, 25)
      Caribbeanextent <- as(raster::extent(Caribbean), "SpatialPolygons") %>%
        st_as_sf()

      GAK <- c(-157.5, -133.1, 50.5, 60)
      GAKextent <- as(raster::extent(GAK), "SpatialPolygons") %>%
        st_as_sf()

      Nino <- c(-170, -120, -5, 5)
      Ninoextent <- as(raster::extent(Nino), "SpatialPolygons") %>%
        st_as_sf()

      Caribbeandat <- crop(sub, extent(Caribbeanextent))
      Caribbeandat <- mask(Caribbeandat, Caribbeanextent)

      GAKdat <- crop(sub, extent(GAKextent))
      GAKdat <- mask(GAKdat,GAKextent)

      Ninodat <- crop(sub, extent(Ninoextent))
      Ninodat <- mask(Ninodat, Ninoextent)

     avg_CSST<- c(avg_CSST, mean(terra::values(Caribbeandat), na.rm = TRUE))
     avg_GAKSST<- c(avg_GAKSST, mean(terra::values(GAKdat), na.rm = TRUE))
     avg_Nino34 <- c(avg_Nino34, mean(terra::values(Ninodat), na.rm = TRUE))
  
     max_CSST<- c(max_CSST, max(terra::values(Caribbeandat), na.rm = TRUE))
     max_GAKSST<- c(max_GAKSST, max(terra::values(GAKdat), na.rm = TRUE))
     max_Nino34 <- c(max_Nino34, max(terra::values(Ninodat), na.rm = TRUE))
  
     min_CSST<- c(min_CSST, min(terra::values(Caribbeandat), na.rm = TRUE))
     min_GAKSST<- c(min_GAKSST, min(terra::values(GAKdat), na.rm = TRUE))
     min_Nino34 <- c(min_Nino34, min(terra::values(Ninodat), na.rm = TRUE))
    }
}
```

```{r}
avg_CSST_2 <- avg_CSST
avg_GAKSST_2 <- avg_GAKSST
avg_Nino34_2 <- avg_Nino34
max_CSST_2 <- max_CSST
max_GAKSST_2 <- max_GAKSST
max_Nino34_2 <- max_Nino34
min_CSST_2 <- min_CSST
min_GAKSST_2 <- min_GAKSST
min_Nino34_2 <- min_Nino34
```

```{r}
avg_CSST_2 <- append(avg_CSST_2, avg_CSST)
avg_GAKSST_2 <- append(avg_GAKSST_2,avg_GAKSST)
avg_Nino34_2 <- append(avg_Nino34_2,avg_Nino34)
max_CSST_2 <- append(max_CSST_2,max_CSST)
max_GAKSST_2 <- append(max_GAKSST_2,max_GAKSST)
max_Nino34_2 <- append(max_Nino34_2,max_Nino34)
min_CSST_2 <- append(min_CSST_2,min_CSST)
min_GAKSST_2 <- append(min_GAKSST_2,min_GAKSST)
min_Nino34_2 <- append(min_Nino34_2,min_Nino34)
```

```{r, eval = FALSE}
ClimVars <- cbind(ClimVars, avg_CSST_2, avg_GAKSST_2, avg_Nino34_2, max_CSST_2, max_GAKSST_2, max_Nino34_2, min_CSST_2, min_GAKSST_2, min_Nino34_2)
```

```{r}
#save(All_Tornadoes, BigDayCentroids.df, BigDays.sfdfT, BigDayTornadoes, Groups.sfdfT, ClimVars, file = "ClusterData.RData")

#save(All_Tornadoes, BigDayCentroids.df, BigDays.sfdfT, BigDayTornadoes, Groups.sfdfT, ClimVars, file = "ClusterData_Backup.RData")
```

```{r, eval = FALSE}
BigDays.sfdfT$DayofYear <- yday(BigDays.sfdfT$cDate)

test <- BigDays.sfdfT[1:5,]

avg_CSST <- numeric()
avg_GAKSST <- numeric()
avg_Nino34 <- numeric()
max_CSST <- numeric()
max_GAKSST <- numeric()
max_Nino34 <- numeric()
min_CSST <- numeric()
min_GAKSST <- numeric()
min_Nino34 <- numeric()
for (i in 1:dim(BigDays.sfdfT)[1]){
  print(i)
  year = BigDays.sfdfT$Year[i]
  day = BigDays.sfdfT$DayofYear[i]
  newdat <- ClimVars %>% 
    filter(Year == year & Day == day)
  avg_CSST <- c(avg_CSST, newdat$avg_CSST)
  avg_GAKSST <- c(avg_GAKSST, newdat$avg_GAKSST)
  avg_Nino34 <- c(avg_Nino34, newdat$avg_Nino34)
  max_CSST <- c(max_CSST, newdat$max_CSST)
  max_GAKSST <- c(max_GAKSST, newdat$max_GAKSST)
  max_Nino34 <- c(max_Nino34, newdat$max_Nino34)
  min_CSST <- c(min_CSST, newdat$min_CSST)
  min_GAKSST <- c(min_GAKSST, newdat$min_GAKSST)
  min_Nino34 <- c(min_Nino34, newdat$min_Nino34)
}
```

```{r, eval = FALSE}
BigDays.sfdfT <- cbind(BigDays.sfdfT, avg_CSST, avg_GAKSST, avg_Nino34, max_CSST, max_GAKSST, max_Nino34, min_CSST, min_GAKSST, min_Nino34)
```

*Get the 5 day average values*
```{r, eval = FALSE}
avg_CSST5day <- numeric()
avg_GAKSST5day <- numeric()
avg_Nino345day <- numeric()
max_CSST5day <- numeric()
max_GAKSST5day <- numeric()
max_Nino345day <- numeric()
min_CSST5day <- numeric()
min_GAKSST5day <- numeric()
min_Nino345day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(ClimVars$Year == BigDays.sfdfT$Year[i] & ClimVars$Day == BigDays.sfdfT$DayofYear[i])
  startrow = rownum-5
  dat <- ClimVars[startrow:rownum,]
  avg_CSST5day <- c(avg_CSST5day, mean(dat$avg_CSST))
  max_CSST5day <- c(max_CSST5day, mean(dat$max_CSST))
  min_CSST5day <- c(min_CSST5day, mean(dat$min_CSST))
  avg_GAKSST5day <- c(avg_GAKSST5day, mean(dat$avg_GAKSST))
  max_GAKSST5day <- c(max_GAKSST5day, mean(dat$max_GAKSST))
  min_GAKSST5day <- c(min_GAKSST5day, mean(dat$min_GAKSST))
  avg_Nino345day <- c(avg_Nino345day, mean(dat$avg_Nino34))
  max_Nino345day <- c(max_Nino345day, mean(dat$max_Nino34))
  min_Nino345day <- c(min_Nino345day, mean(dat$min_Nino34))
} 

BigDays.sfdfT <- cbind(BigDays.sfdfT, avg_CSST5day, avg_GAKSST5day, avg_Nino345day, max_CSST5day, max_GAKSST5day, max_Nino345day, min_CSST5day, min_GAKSST5day, min_Nino345day)

```

*Get the 10 day average values*
```{r, eval = FALSE}
avg_CSST10day <- numeric()
avg_GAKSST10day <- numeric()
avg_Nino3410day <- numeric()
max_CSST10day <- numeric()
max_GAKSST10day <- numeric()
max_Nino3410day <- numeric()
min_CSST10day <- numeric()
min_GAKSST10day <- numeric()
min_Nino3410day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(ClimVars$Year == BigDays.sfdfT$Year[i] & ClimVars$Day == BigDays.sfdfT$DayofYear[i])
  startrow = rownum-10
  dat <- ClimVars[startrow:rownum,]
  avg_CSST10day <- c(avg_CSST10day, mean(dat$avg_CSST))
  max_CSST10day <- c(max_CSST10day, mean(dat$max_CSST))
  min_CSST10day <- c(min_CSST10day, mean(dat$min_CSST))
  avg_GAKSST10day <- c(avg_GAKSST10day, mean(dat$avg_GAKSST))
  max_GAKSST10day <- c(max_GAKSST10day, mean(dat$max_GAKSST))
  min_GAKSST10day <- c(min_GAKSST10day, mean(dat$min_GAKSST))
  avg_Nino3410day <- c(avg_Nino3410day, mean(dat$avg_Nino34))
  max_Nino3410day <- c(max_Nino3410day, mean(dat$max_Nino34))
  min_Nino3410day <- c(min_Nino3410day, mean(dat$min_Nino34))
} 

BigDays.sfdfT <- cbind(BigDays.sfdfT, avg_CSST10day, avg_GAKSST10day, avg_Nino3410day, max_CSST10day, max_GAKSST10day, max_Nino3410day, min_CSST10day, min_GAKSST10day, min_Nino3410day)

```
*Get the 15 day average values*
```{r, eval = FALSE}
avg_CSST15day <- numeric()
avg_GAKSST15day <- numeric()
avg_Nino3415day <- numeric()
max_CSST15day <- numeric()
max_GAKSST15day <- numeric()
max_Nino3415day <- numeric()
min_CSST15day <- numeric()
min_GAKSST15day <- numeric()
min_Nino3415day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(ClimVars$Year == BigDays.sfdfT$Year[i] & ClimVars$Day == BigDays.sfdfT$DayofYear[i])
  startrow = rownum-15
  dat <- ClimVars[startrow:rownum,]
  avg_CSST15day <- c(avg_CSST15day, mean(dat$avg_CSST))
  max_CSST15day <- c(max_CSST15day, mean(dat$max_CSST))
  min_CSST15day <- c(min_CSST15day, mean(dat$min_CSST))
  avg_GAKSST15day <- c(avg_GAKSST15day, mean(dat$avg_GAKSST))
  max_GAKSST15day <- c(max_GAKSST15day, mean(dat$max_GAKSST))
  min_GAKSST15day <- c(min_GAKSST15day, mean(dat$min_GAKSST))
  avg_Nino3415day <- c(avg_Nino3415day, mean(dat$avg_Nino34))
  max_Nino3415day <- c(max_Nino3415day, mean(dat$max_Nino34))
  min_Nino3415day <- c(min_Nino3415day, mean(dat$min_Nino34))
} 

BigDays.sfdfT <- cbind(BigDays.sfdfT, avg_CSST15day, avg_GAKSST15day, avg_Nino3415day, max_CSST15day, max_GAKSST15day, max_Nino3415day, min_CSST15day, min_GAKSST15day, min_Nino3415day)

```
*Get the 20 day average values*
```{r, eval = FALSE}
avg_CSST20day <- numeric()
avg_GAKSST20day <- numeric()
avg_Nino3420day <- numeric()
max_CSST20day <- numeric()
max_GAKSST20day <- numeric()
max_Nino3420day <- numeric()
min_CSST20day <- numeric()
min_GAKSST20day <- numeric()
min_Nino3420day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(ClimVars$Year == BigDays.sfdfT$Year[i] & ClimVars$Day == BigDays.sfdfT$DayofYear[i])
  startrow = rownum-20
  dat <- ClimVars[startrow:rownum,]
  avg_CSST20day <- c(avg_CSST20day, mean(dat$avg_CSST))
  max_CSST20day <- c(max_CSST20day, mean(dat$max_CSST))
  min_CSST20day <- c(min_CSST20day, mean(dat$min_CSST))
  avg_GAKSST20day <- c(avg_GAKSST20day, mean(dat$avg_GAKSST))
  max_GAKSST20day <- c(max_GAKSST20day, mean(dat$max_GAKSST))
  min_GAKSST20day <- c(min_GAKSST20day, mean(dat$min_GAKSST))
  avg_Nino3420day <- c(avg_Nino3420day, mean(dat$avg_Nino34))
  max_Nino3420day <- c(max_Nino3420day, mean(dat$max_Nino34))
  min_Nino3420day <- c(min_Nino3420day, mean(dat$min_Nino34))
} 

BigDays.sfdfT <- cbind(BigDays.sfdfT, avg_CSST20day, avg_GAKSST20day, avg_Nino3420day, max_CSST20day, max_GAKSST20day, max_Nino3420day, min_CSST20day, min_GAKSST20day, min_Nino3420day)

```

*Get the 30 day average values*
```{r, eval = FALSE}
avg_CSST30day <- numeric()
avg_GAKSST30day <- numeric()
avg_Nino3430day <- numeric()
max_CSST30day <- numeric()
max_GAKSST30day <- numeric()
max_Nino3430day <- numeric()
min_CSST30day <- numeric()
min_GAKSST30day <- numeric()
min_Nino3430day <- numeric()

for (i in 1:dim(BigDays.sfdfT)[1]) {
  print(i)
  rownum = which(ClimVars$Year == BigDays.sfdfT$Year[i] & ClimVars$Day == BigDays.sfdfT$DayofYear[i])
  startrow = rownum-30
  dat <- ClimVars[startrow:rownum,]
  avg_CSST30day <- c(avg_CSST30day, mean(dat$avg_CSST))
  max_CSST30day <- c(max_CSST30day, mean(dat$max_CSST))
  min_CSST30day <- c(min_CSST30day, mean(dat$min_CSST))
  avg_GAKSST30day <- c(avg_GAKSST30day, mean(dat$avg_GAKSST))
  max_GAKSST30day <- c(max_GAKSST30day, mean(dat$max_GAKSST))
  min_GAKSST30day <- c(min_GAKSST30day, mean(dat$min_GAKSST))
  avg_Nino3430day <- c(avg_Nino3430day, mean(dat$avg_Nino34))
  max_Nino3430day <- c(max_Nino3430day, mean(dat$max_Nino34))
  min_Nino3430day <- c(min_Nino3430day, mean(dat$min_Nino34))
} 

BigDays.sfdfT <- cbind(BigDays.sfdfT, avg_CSST30day, avg_GAKSST30day, avg_Nino3430day, max_CSST30day, max_GAKSST30day, max_Nino3430day, min_CSST30day, min_GAKSST30day, min_Nino3430day)

```

```{r}
#save(All_Tornadoes, BigDayCentroids.df, BigDays.sfdfT, BigDayTornadoes, Groups.sfdfT, ClimVars, file = "ClusterData.RData")

#save(All_Tornadoes, BigDayCentroids.df, BigDays.sfdfT, BigDayTornadoes, Groups.sfdfT, ClimVars, file = "ClusterData_Backup.RData")
```

```{r, eval = FALSE}
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.1994.nc" = "AirTemp1994.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.1995.nc", destfile= "AirTemp1995.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.1996.nc", destfile= "AirTemp1996.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.1997.nc", destfile= "AirTemp1997.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.1998.nc", destfile= "AirTemp1998.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.1999.nc", destfile= "AirTemp1999.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2000.nc", destfile= "AirTemp2000.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2001.nc", destfile= "AirTemp12001nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2002.nc", destfile= "AirTemp2002.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2003.nc", destfile= "AirTemp2003.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2004.nc", destfile= "AirTemp2004.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2005.nc", destfile= "AirTemp2005.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2006.nc", destfile= "AirTemp2006.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2008.nc", destfile= "AirTemp2008.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2009.nc", destfile= "AirTemp2009.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2007.nc", destfile= "AirTemp2007.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2010.nc", destfile= "AirTemp2010.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2011.nc", destfile= "AirTemp2011.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2012.nc", destfile= "AirTemp2012.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2013.nc", destfile= "AirTemp2013.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2014.nc", destfile= "AirTemp2014.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2015.nc", destfile= "AirTemp2015.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2016.nc", destfile= "AirTemp2016.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2017.nc", destfile= "AirTemp2017.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2018.nc", destfile= "AirTemp2018.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2019.nc", destfile= "AirTemp2019.nc", mode = "wb")
#download.file(url ="ftp://ftp2.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.2020.nc", destfile= "AirTemp2020.nc", mode = "wb")
```
